{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WPAV7zw1dszh"
   },
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VTtLznL4dszn"
   },
   "source": [
    "## Marta ---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9aJCqXMUdszn"
   },
   "source": [
    "put all of your code between here and the next person's name only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KPeI3XU1dszo"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Lasso, LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import plot_confusion_matrix, recall_score, accuracy_score, f1_score, make_scorer, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X_GMPCiQm8EY",
    "outputId": "1067adff-4b85-419e-dc89-2dd17f6f5716"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_F7hQyHWoFfi"
   },
   "outputs": [],
   "source": [
    "#with open('/content/drive/My Drive/Colab Notebooks/mdi_df_yr_trail.csv', 'r') as data:\n",
    "#  noaa_on_fire = pd.read_csv(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DcubP-XpotkX"
   },
   "outputs": [],
   "source": [
    "noaa_on_fire =  pd.read_csv('/content/drive/My Drive/Colab Notebooks/mdi_df_yr_trail.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zevN-qtFdszp"
   },
   "outputs": [],
   "source": [
    "noaa_on_fire = pd.get_dummies(noaa_on_fire, columns=['state', 'month'], drop_first= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-Js_xUo2dszp",
    "outputId": "ad742209-d6d2-4983-8a9c-fa681bab8874"
   },
   "outputs": [],
   "source": [
    "noaa_on_fire.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2F-syCNgdszq",
    "outputId": "eebf26c0-6ff5-4b3e-dc5e-820d26695265"
   },
   "outputs": [],
   "source": [
    "noaa_on_fire['stat_cause_descr'].unique()\n",
    "not_stupidity = ['Miscellaneous',  'Railroad', 'Powerline', 'Lightning', 'Missing/Undefined', 'Fireworks']\n",
    "noaa_on_fire['caused_by_stupidity'] = noaa_on_fire['stat_cause_descr'].apply(lambda x: 0 if x in not_stupidity else 1)\n",
    "noaa_on_fire['caused_by_stupidity'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BbW7spO0dszq"
   },
   "outputs": [],
   "source": [
    "noaa_on_fire = noaa_on_fire[noaa_on_fire['caused_by_stupidity'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dU3qi4OSdszq"
   },
   "outputs": [],
   "source": [
    "X_vars = ['state_CA', 'state_CO', 'state_ID',\n",
    "       'state_MT', 'state_NM', 'state_NV', 'state_OR', 'state_UT', 'state_WA',\n",
    "       'state_WY', 'month_2', 'month_3', 'month_4', 'month_5', 'month_6',\n",
    "       'month_7', 'month_8', 'month_9', 'month_10', 'month_11', 'month_12','tavg_t3m', 'pcp_t12m', 'pmdi_t12m']\n",
    "\n",
    "noaa_on_fire = noaa_on_fire.dropna(subset = X_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BXveoMjBdszq"
   },
   "outputs": [],
   "source": [
    "y = noaa_on_fire['fire_size']\n",
    "\n",
    "X = noaa_on_fire[X_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fG8noXbBdszr"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "\n",
    "stan = StandardScaler()\n",
    "X_train = stan.fit_transform(X_train)\n",
    "X_test = stan.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FkrcYi7odszr"
   },
   "source": [
    "REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vhht9-YHdszr"
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lasso = Lasso()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N0yPKOo-dszr",
    "outputId": "fc887995-fde4-45e8-97fe-e87591e91fb1"
   },
   "outputs": [],
   "source": [
    "for q in [0, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5]:\n",
    "    print(f\"Using data corresponding to fire size > {q} quantile:\")\n",
    "    noaa_on_fire = noaa_on_fire[noaa_on_fire['fire_size']> noaa_on_fire['fire_size'].quantile(q)]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "\n",
    "    stan = StandardScaler()\n",
    "    X_train = stan.fit_transform(X_train)\n",
    "    X_test = stan.transform(X_test)\n",
    "    \n",
    "    lr = LinearRegression()\n",
    "    lasso = Lasso()\n",
    "    \n",
    "    lr.fit(X_train, y_train)\n",
    "    lasso.fit(X_train, y_train)\n",
    "    print(f\"Linear Regression achieves R2 of {round(lr.score(X_train, y_train),4)} on train data and {round(lr.score(X_test, y_test),4)} on test data.\")\n",
    "    print(f\"Lasso Regression achieves R2 of {round(lasso.score(X_train, y_train),4)} on train data and {round(lasso.score(X_test, y_test),4)} on test data.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c4puctQ_dszs"
   },
   "source": [
    "Note: best LR with all quantiled data has R2 of 0.0016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BD_HWAtgdszs"
   },
   "source": [
    "CLASSIFICATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SAsC1fIJdszs"
   },
   "outputs": [],
   "source": [
    "X_vars = ['state_CA', 'state_CO', 'state_ID',\n",
    "       'state_MT', 'state_NM', 'state_NV', 'state_OR', 'state_UT', 'state_WA',\n",
    "       'state_WY', 'month_2', 'month_3', 'month_4', 'month_5', 'month_6',\n",
    "       'month_7', 'month_8', 'month_9', 'month_10', 'month_11', 'month_12','tavg_t12m',\n",
    "       'tavg_t9m', 'tavg_t6m', 'tavg_t3m', 'pcp_t12m', 'pcp_t9m', 'pcp_t6m',\n",
    "       'pcp_t3m', 'pmdi_t12m', 'pmdi_t9m', 'pmdi_t6m', 'pmdi_t3m', 'pdsi_t12m',\n",
    "       'pdsi_t9m', 'pdsi_t6m', 'pdsi_t3m']\n",
    "\n",
    "noaa_on_fire = noaa_on_fire.dropna(subset = X_vars)\n",
    "\n",
    "X = noaa_on_fire[X_vars]\n",
    "y = noaa_on_fire['fire_size_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "msIW6duvdszs"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y)\n",
    "\n",
    "stan = StandardScaler()\n",
    "X_train = stan.fit_transform(X_train)\n",
    "X_test = stan.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LX9QjelMdszs",
    "outputId": "ca73394c-ced0-4fdd-ad51-e2f4923cb56d"
   },
   "outputs": [],
   "source": [
    "logr = LogisticRegression(max_iter = 10000)\n",
    "logr.fit(X_train, y_train)\n",
    "print(f\"Logistic Regression achieves accuracy of {round(logr.score(X_train, y_train),4)} on train data and {round(logr.score(X_test, y_test),4)} on test data.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_yDyzlpv1eNX",
    "outputId": "a5daee2e-a6fc-4296-85ed-07941039f661"
   },
   "outputs": [],
   "source": [
    "recall_score(y_train, logr.predict(X_train), average = 'weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "IPDz1N2ddszt",
    "outputId": "36a222c7-d801-4e00-d4f9-994cb7875605"
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(logr, X_test, y_test, cmap = 'YlOrBr', normalize= 'true');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ULRo9lZdszt"
   },
   "source": [
    "REGRESSION WITH BOOTSTRAPPING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcYN-tY7dszt"
   },
   "source": [
    "Adding more representation of classes C, D, ..., G by including re-sampled observations with fire_size > 10 (which corresponds to those under-represented classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ol-vz-ddszt"
   },
   "outputs": [],
   "source": [
    "noaa_on_fire_btstrp = pd.concat([noaa_on_fire, \n",
    "                                 noaa_on_fire[noaa_on_fire['fire_size_class'] == 'C'].sample(n = 100000, replace = True, random_state=11),\n",
    "                                 noaa_on_fire[noaa_on_fire['fire_size_class'] == 'D'].sample(n = 100000, replace = True, random_state=11),\n",
    "                                 noaa_on_fire[noaa_on_fire['fire_size_class'] == 'E'].sample(n = 100000, replace = True, random_state=11),\n",
    "                                 noaa_on_fire[noaa_on_fire['fire_size_class'] == 'F'].sample(n = 100000, replace = True, random_state=11),\n",
    "                                 noaa_on_fire[noaa_on_fire['fire_size_class'] == 'G'].sample(n = 100000, replace = True, random_state=11)], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f9YzLSzNvUJF"
   },
   "outputs": [],
   "source": [
    "noaa_on_fire = noaa_on_fire.dropna(subset = X_vars)\n",
    "\n",
    "X_vars = ['state_CA', 'state_CO', 'state_ID',\n",
    "       'state_MT', 'state_NM', 'state_NV', 'state_OR', 'state_UT', 'state_WA',\n",
    "       'state_WY', 'month_2', 'month_3', 'month_4', 'month_5', 'month_6',\n",
    "       'month_7', 'month_8', 'month_9', 'month_10', 'month_11', 'month_12','tavg_t12m',\n",
    "       'tavg_t9m', 'tavg_t6m', 'tavg_t3m', 'pcp_t12m', 'pcp_t9m', 'pcp_t6m',\n",
    "       'pcp_t3m', 'pmdi_t12m', 'pmdi_t9m', 'pmdi_t6m', 'pmdi_t3m', 'pdsi_t12m',\n",
    "       'pdsi_t9m', 'pdsi_t6m', 'pdsi_t3m', 'zndx', 'cdd', 'hdd']\n",
    "X = noaa_on_fire_btstrp[X_vars]\n",
    "y = noaa_on_fire_btstrp['fire_size_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YG7uJ3Xadszu"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, stratify = y)\n",
    "\n",
    "stan = StandardScaler()\n",
    "X_train = stan.fit_transform(X_train)\n",
    "X_test = stan.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DZReL1ERdszu",
    "outputId": "354f5f23-70cb-4945-9b04-57645bb07928"
   },
   "outputs": [],
   "source": [
    "logr_b = LogisticRegression(max_iter = 10000)\n",
    "logr_b.fit(X_train, y_train)\n",
    "print(f\"Logistic Regression achieves accuracy of {round(logr_b.score(X_train, y_train),4)} on train data and {round(logr_b.score(X_test, y_test),4)} on test data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "49mUYz8odszv",
    "outputId": "465b7a7f-05f0-4902-eee6-a768315eca41"
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(logr_b, X_test, y_test,cmap = 'YlOrBr', normalize = 'true');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIRIr1Zzx6o7"
   },
   "source": [
    "Applying class weights to stress importance of G, F, E, D relative to smaller fires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QVWfRvNnx5Bn",
    "outputId": "167f30ba-08ea-4637-839e-109d4fbf7506"
   },
   "outputs": [],
   "source": [
    "LogisticRegression(class_weight = {'A':0.05, 'B': 0.05, 'C': 0.05, 'D': 0.1, 'E': 0.2, 'F': 0.25, 'G': 0.3},  max_iter = 10000, verbose = 1)\n",
    "logr_w = LogisticRegression(max_iter = 10000)\n",
    "logr_w.fit(X_train, y_train)\n",
    "print(f\"Logistic Regression achieves accuracy of {round(logr_w.score(X_train, y_train),4)} on train data and {round(logr_w.score(X_test, y_test),4)} on test data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tiSh7Gck1Kju",
    "outputId": "adbdeb1e-f376-45ed-9c9c-9b0c86d8d8ae"
   },
   "outputs": [],
   "source": [
    "f1_score(y_train, logr_w.predict(X_train), average = 'weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dvr3EpwmzxF7",
    "outputId": "f57aa675-c7b4-4865-8970-9b03489bcb82"
   },
   "outputs": [],
   "source": [
    "print(f\"Logistic Regression achieves recall of {round(recall_score(y_train, logr_w.predict(X_train), average = 'weighted'),4)} on train data and {round(recall_score(y_test, logr_w.predict(X_test), average = 'weighted'),4)} on test data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "id": "OJlNUD8lyPyv",
    "outputId": "61cc7795-983a-4f46-ba57-9bc6d3062bef"
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(logr_w, X_test, y_test,cmap = 'YlOrBr', normalize = 'true');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUEp28i5uSQ0"
   },
   "source": [
    "Tuning parameters to improve accuracy (recall?) - *couldn't afford to let this run to completion...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jtSrVLMr4gWv",
    "outputId": "f4b062ab-a982-4010-a780-08814e151285"
   },
   "outputs": [],
   "source": [
    "logr_w.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wf8nKlBAvKWV"
   },
   "outputs": [],
   "source": [
    "params= {\n",
    "    'C': np.linspace(0,1,5), \n",
    "    'solver': ['lbfgs','newton-cg', 'lbfgs', 'sag', 'saga'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 358
    },
    "id": "KwgxFhmy6Mne",
    "outputId": "a79f2f09-ff77-4dbc-d667-5d7550ce59b2"
   },
   "outputs": [],
   "source": [
    "logr_gs = GridSearchCV(logr_w, param_grid = params, cv = 5, verbose = 1, n_jobs = -1 )\n",
    "logr_gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hOiYx4qJay1"
   },
   "source": [
    "Changing the objective to \"large\" fires:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BPISvKEJdszu"
   },
   "outputs": [],
   "source": [
    "noaa_on_fire_btstrp['large'] = noaa_on_fire_btstrp['fire_size'].apply(lambda x: 1 if x > 100 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e32de8g4dszu"
   },
   "outputs": [],
   "source": [
    "y = noaa_on_fire_btstrp['large']\n",
    "\n",
    "X_vars = ['state_CA', 'state_CO', 'state_ID',\n",
    "       'state_MT', 'state_NM', 'state_NV', 'state_OR', 'state_UT', 'state_WA',\n",
    "       'state_WY', 'month_2', 'month_3', 'month_4', 'month_5', 'month_6',\n",
    "       'month_7', 'month_8', 'month_9', 'month_10', 'month_11', 'month_12','tavg_t12m',\n",
    "       'tavg_t9m', 'tavg_t6m', 'tavg_t3m', 'pcp_t12m', 'pcp_t9m', 'pcp_t6m',\n",
    "       'pcp_t3m', 'pmdi_t12m', 'pmdi_t9m', 'pmdi_t6m', 'pmdi_t3m', 'pdsi_t12m',\n",
    "       'pdsi_t9m', 'pdsi_t6m', 'pdsi_t3m', 'zndx', 'cdd', 'hdd']\n",
    "X = noaa_on_fire_btstrp[X_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xevwpU2ldszw",
    "outputId": "fb3e1594-8628-4894-8867-f0993ba0c051"
   },
   "outputs": [],
   "source": [
    "large_gts = [5, 10, 100, 300, 1000]\n",
    "\n",
    "for l in large_gts:\n",
    "    noaa_on_fire_btstrp['large'] = noaa_on_fire_btstrp['fire_size'].apply(lambda x: 1 if x > l else 0)\n",
    "\n",
    "    y = noaa_on_fire_btstrp['large']\n",
    "\n",
    "    X_vars = ['state_CA', 'state_CO', 'state_ID',\n",
    "           'state_MT', 'state_NM', 'state_NV', 'state_OR', 'state_UT', 'state_WA',\n",
    "           'state_WY', 'month_2', 'month_3', 'month_4', 'month_5', 'month_6',\n",
    "           'month_7', 'month_8', 'month_9', 'month_10', 'month_11', 'month_12','tavg_t12m',\n",
    "           'tavg_t9m', 'tavg_t6m', 'tavg_t3m', 'pcp_t12m', 'pcp_t9m', 'pcp_t6m',\n",
    "           'pcp_t3m', 'pmdi_t12m', 'pmdi_t9m', 'pmdi_t6m', 'pmdi_t3m', 'pdsi_t12m',\n",
    "           'pdsi_t9m', 'pdsi_t6m', 'pdsi_t3m', 'zndx', 'cdd', 'hdd']\n",
    "    \n",
    "    X = noaa_on_fire_btstrp[X_vars]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, stratify = y)\n",
    "\n",
    "    stan = StandardScaler()\n",
    "    X_train = stan.fit_transform(X_train)\n",
    "    X_test = stan.transform(X_test)\n",
    "\n",
    "    logr_b = LogisticRegression(max_iter = 10000)\n",
    "    logr_b.fit(X_train, y_train)\n",
    "    print(f\"When detecting fires > {l} acres, Logistic Regression achieves accuracy of {round(logr_b.score(X_train, y_train),4)} on train data and {round(logr_b.score(X_test, y_test),4)} on test data.\")\n",
    "    print(f\"The model's recall is {round(recall_score(y_train, logr_b.predict(X_train)),4)} on train data and {round(recall_score(y_test, logr_b.predict(X_test),4))} on test data.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6XF0lNNwdszw",
    "outputId": "da383107-30a6-4be7-9033-7d0aa1208989"
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(logr_b, X_test, y_test,cmap = 'YlOrBr');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eBUtCxgadszw",
    "outputId": "20fe2c67-8000-41d5-9051-dc3fb1fc20b6"
   },
   "outputs": [],
   "source": [
    "recall_score(y_test, logr_b.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fYPrJWVLJlYR"
   },
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SyjWS1zkJyJj"
   },
   "outputs": [],
   "source": [
    "X_vars = ['state_CA', 'state_CO', 'state_ID',\n",
    "       'state_MT', 'state_NM', 'state_NV', 'state_OR', 'state_UT', 'state_WA',\n",
    "       'state_WY', 'month_2', 'month_3', 'month_4', 'month_5', 'month_6',\n",
    "       'month_7', 'month_8', 'month_9', 'month_10', 'month_11', 'month_12','tavg_t12m',\n",
    "       'tavg_t9m', 'tavg_t6m', 'tavg_t3m', 'pcp_t12m', 'pcp_t9m', 'pcp_t6m',\n",
    "       'pcp_t3m', 'pmdi_t12m', 'pmdi_t9m', 'pmdi_t6m', 'pmdi_t3m', 'pdsi_t12m',\n",
    "       'pdsi_t9m', 'pdsi_t6m', 'pdsi_t3m', 'zndx', 'cdd', 'hdd']\n",
    "\n",
    "noaa_on_fire = noaa_on_fire.dropna(subset = X_vars)\n",
    "\n",
    "X = noaa_on_fire_btstrp[X_vars]\n",
    "y = noaa_on_fire_btstrp['fire_size_class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k26k4A2SJyJ1"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y, stratify = y)\n",
    "\n",
    "stan = StandardScaler()\n",
    "X_train = stan.fit_transform(X_train)\n",
    "X_test = stan.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XqZeCBrUJpqz"
   },
   "outputs": [],
   "source": [
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "print(f\"Support Vector Classifier achieves accuracy of {round(svc.score(X_train, y_train),4)} on train data and {round(svc.score(X_test, y_test),4)} on test data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bPyBoaNIKAm5"
   },
   "outputs": [],
   "source": [
    "plot_confusion_matrix(svc, X_test, y_test,cmap = 'YlOrBr', normalize = 'true');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1OXhcvKCrqzU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LmgjvvVarqpf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WALhMyCqrqdK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8TQckKUZrqI2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jRUpNQ_sdszw"
   },
   "source": [
    "## Jesse ---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42) \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import plot_confusion_matrix, recall_score, accuracy_score, f1_score, make_scorer, precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nTAqVZdRdszx"
   },
   "outputs": [],
   "source": [
    "# read dataset\n",
    "model_df = pd.read_csv('data/mfi_df_yr.csv')\n",
    "\n",
    "# Drop Unnamed: 0\n",
    "model_df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "#examine\n",
    "model_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#credit to Marta\n",
    "model_df = pd.concat([model_df, \n",
    "                                 model_df[model_df['fire_size_class'] == 'C'].sample(n = 100000, replace = True, random_state=42),\n",
    "                                 model_df[model_df['fire_size_class'] == 'D'].sample(n = 100000, replace = True, random_state=42),\n",
    "                                 model_df[model_df['fire_size_class'] == 'E'].sample(n = 100000, replace = True, random_state=42),\n",
    "                                 model_df[model_df['fire_size_class'] == 'F'].sample(n = 100000, replace = True, random_state=42),\n",
    "                                 model_df[model_df['fire_size_class'] == 'G'].sample(n = 100000, replace = True, random_state=42)],\n",
    "                     axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XwqblQAddszx"
   },
   "source": [
    "### One hot encode variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = pd.get_dummies(model_df, columns=['month'], drop_first=True) # in relation to January\n",
    "model_df = pd.get_dummies(model_df, columns=['statecode'], drop_first=True) # location matters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create X, y and train, test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df['fire_size_class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I don't want to dummy neighboorhood, but I do want to bin it into oridinal columns based on mean price\n",
    "model_df['fire_size_class_order'] = model_df['fire_size_class']\n",
    "model_df_ordinal_str_columns = model_df[['fire_size_class_order']]\n",
    "\n",
    "dict_ordinal = {\n",
    "    'A': 1,\n",
    "    'B' : 2,\n",
    "    'C' : 3,\n",
    "    'D': 4,\n",
    "    'E' : 5,\n",
    "    'F' : 6,\n",
    "    'G' : 7,\n",
    "      }\n",
    "\n",
    "#loop to get the job done\n",
    "for i in model_df_ordinal_str_columns.columns:\n",
    "    model_df.replace({i: dict_ordinal},inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jPvFXZtkdszx"
   },
   "outputs": [],
   "source": [
    "#create X and y variables\n",
    "X = model_df[[ 'pcp', 'tavg', 'pdsi', 'phdi', 'zndx', 'pmdi', 'sp02',\n",
    "       'sp03', 'sp06', 'sp09', 'sp12', 'sp24', 'tmin', 'tmax', 'month_2', 'month_3',\n",
    "       'month_4', 'month_5', 'cdd', 'hdd','month_6', 'month_7', 'month_8', 'month_9',\n",
    "       'month_10', 'month_11', 'month_12','statecode_4', 'statecode_5', 'statecode_10',\n",
    "       'statecode_24', 'statecode_26', 'statecode_29', 'statecode_35',\n",
    "       'statecode_45', 'statecode_110']]\n",
    "y = model_df[['fire_size_class_order']]\n",
    "\n",
    "assert len(X) == len(y)\n",
    "\n",
    "#split into training and test groups\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42, test_size=.2, train_size = .8, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare y for confusion matrix\n",
    "y_train_con = y_train\n",
    "y_test_con = y_test\n",
    "\n",
    "# prepare y for multiclass modeling\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "X_train_sc = ss.fit_transform(X_train)\n",
    "X_test_sc = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTION: Add polynomial features to capture non-linear relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = PolynomialFeatures(degree=2)\n",
    "X_train_sc = transform.fit_transform(X_train_sc)\n",
    "X_test_sc = transform.fit_transform(X_test_sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OPTION: Principle Component Analysis to reduce dimensionality of polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate PCA with 30 components.\n",
    "pca = PCA(n_components=30, random_state=42)\n",
    "\n",
    "# Fit PCA to training data.\n",
    "pca.fit(X_train_sc)\n",
    "\n",
    "# Transform Z_train and Z_test.\n",
    "X_train_sc = pca.transform(X_train_sc)\n",
    "X_test_sc = pca.transform(X_test_sc)\n",
    "\n",
    "# Pull the explained variance attribute.\n",
    "var_exp = pca.explained_variance_ratio_\n",
    "print(f'Explained variance (first 30 components): {np.round(var_exp[:20], 3)}')\n",
    "\n",
    "print('')\n",
    "\n",
    "# Generate the cumulative explained variance.\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "print(f'Cumulative explained variance (first 30 components): {np.round(cum_var_exp[:20], 3)}')\n",
    "\n",
    "columns = [f'PCA_{i+1}' for i in pd.DataFrame(X_train_sc).columns]\n",
    "var_ratio_df = pd.DataFrame({'Variation':var_exp,\n",
    "             'PC':columns})\n",
    "var_ratio_df.head()\n",
    "sns.barplot(x=var_ratio_df['PC'].head(10) ,y=var_ratio_df['Variation'].head(10), \n",
    "           data=var_ratio_df, color=\"c\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Topology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate model\n",
    "model = Sequential()\n",
    "\n",
    "#input layer\n",
    "n_input = X_train_sc.shape[1]\n",
    "\n",
    "#dense layers\n",
    "model.add(Dense(n_input, \n",
    "                input_dim=n_input,\n",
    "                activation='relu',\n",
    "                kernel_regularizer=l2(0.001)))\n",
    "model.add(Dense(24, \n",
    "                input_dim=n_input,\n",
    "                activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(16, \n",
    "                input_dim=n_input,\n",
    "                activation='relu',\n",
    "                ))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "#output layer\n",
    "model.add(Dense(8, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit model to dataset\n",
    "history = model.fit(X_train_sc, y_train, validation_data=(X_test_sc, y_test), epochs=50, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize loss\n",
    "train_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(train_loss, label='Training loss', color='navy')\n",
    "plt.plot(test_loss, label='Testing loss', color='skyblue')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history['accuracy'][-1],history.history['val_accuracy'][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='Train accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Val accuracy')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[None, 'micro', 'macro', 'weighted']\n",
    "recall_score(y_test_con, model.predict_classes(X_test_sc), average = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict_classes(X_test_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_mat = tf.math.confusion_matrix(labels=y_test_con, predictions=y_pred).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['0', 'A', 'B', 'C', 'D', 'E', 'F', 'G']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_mat_norm = np.around(con_mat.astype('float') / con_mat.sum(axis=1)[:, np.newaxis], decimals=2)\n",
    "\n",
    "con_mat_df = pd.DataFrame(con_mat_norm,\n",
    "                     index = classes, \n",
    "                     columns = classes)\n",
    "\n",
    "con_mat_df_values = pd.DataFrame(con_mat,\n",
    "                     index = classes, \n",
    "                     columns = classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(8, 8))\n",
    "sns.heatmap(con_mat_df, annot=True,cmap=plt.cm.Blues)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejWbHtqSdszx"
   },
   "source": [
    "## CM ---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1Eap5Xodszx"
   },
   "source": [
    "put all of your code between here and the next person's name only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Q8AgWiOdszx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4eo4M1Ildszy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "owxgK9WKdszy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "By0ndladdszy"
   },
   "source": [
    "## Kira ---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mLe2mUeKdszy"
   },
   "source": [
    "put all of your code between here and the next person's name only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jMO4ZaNodszy"
   },
   "outputs": [],
   "source": [
    "# Import libraries.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q_rngqd7dszy",
    "outputId": "b8c7332a-ca31-44c4-d01c-ee9c6bb4427b"
   },
   "outputs": [],
   "source": [
    "mfi_df = pd.read_csv('data/mfi_df_yr.csv')\n",
    "mfi_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qWYh5OFOdszy",
    "outputId": "eedf0a76-c189-4eac-ff6b-71052eaf6201"
   },
   "outputs": [],
   "source": [
    "mfi_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XaeRzMIFdszz",
    "outputId": "342f3e6d-5cca-44e1-e310-8d1808dfa423"
   },
   "outputs": [],
   "source": [
    "list(mfi_df.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kWeXEVWldszz"
   },
   "outputs": [],
   "source": [
    "# Drop the `Unnamed: 0` column.\n",
    "mfi_df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "\n",
    "# Drop NAs.\n",
    "# df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "It4LanDEdszz"
   },
   "outputs": [],
   "source": [
    "# Create dummies for the `ChestPain`, `Thal`, and `AHD` columns.\n",
    "# Be sure to set `drop_first=True`.\n",
    "mfi_df = pd.get_dummies(mfi_df,\n",
    "                    columns=['stat_cause_descr', 'state'],\n",
    "                    drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uMYW6DVAdszz"
   },
   "outputs": [],
   "source": [
    "mfi_df.drop(columns=['fire_year', 'cont_date_fixed', \n",
    "                        'disc_date_fixed', 'time_burnt', \n",
    "                        'statecode',\n",
    "                        'division', 'yearmonth'],\n",
    "                        axis = 1,\n",
    "                        inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zW3R20PJdszz",
    "outputId": "0b5a0da0-66bf-4f93-affc-af509eb4db12"
   },
   "outputs": [],
   "source": [
    "mfi_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rAPVuiwqdszz",
    "outputId": "e075001e-603e-41ce-89fa-4eb5f42f5b98",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define X and y.\n",
    "X = mfi_df.drop(columns=['fire_size']).select_dtypes(include=['float64'])\n",
    "y = mfi_df['fire_size'].to_numeric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mso0vS_qdszz"
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b1k11NCVdsz0"
   },
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RtrkvGngdsz0"
   },
   "outputs": [],
   "source": [
    "# Split data into training and testing sets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    random_state=42,\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CpFxUkwwdsz0"
   },
   "outputs": [],
   "source": [
    "# What is the accuracy of our baseline model?\n",
    "y.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2gtP6OStdsz0"
   },
   "outputs": [],
   "source": [
    "# Instantiate `RandomForestClassifier` object.\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "et = ExtraTreesClassifier(n_estimators=100)\n",
    "\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L1CEqUVWdsz0"
   },
   "outputs": [],
   "source": [
    "# Fit and score on the training data.\n",
    "cross_val_score(rf, X_train, y_train, cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f-pauVW0dsz0"
   },
   "outputs": [],
   "source": [
    "# Fit and score on the training data.\n",
    "cross_val_score(et, X_train, y_train, cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kC39smR2dsz1"
   },
   "outputs": [],
   "source": [
    "# Score on the testing data.\n",
    "tree.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0utyGPuzdsz1"
   },
   "outputs": [],
   "source": [
    "# Generate one bootstrapped sample\n",
    "# of size n from X_train.\n",
    "\n",
    "X_train.sample(n = X_train.shape[0],\n",
    "               replace = True,\n",
    "               random_state = 42)\n",
    "\n",
    "def bootstrap(data, num_B):\n",
    "    # Create empty list for output.\n",
    "    output = []\n",
    "    \n",
    "    # Generate num_B bootstrapped samples.\n",
    "    for B in range(num_B):\n",
    "        \n",
    "        # Each sample is sampled from data with\n",
    "        # the same sample size as the original\n",
    "        # data, and samples with replacement.\n",
    "        bootstrapped_sample = data.sample(n = data.shape[0],\n",
    "                                        replace = True)\n",
    "        \n",
    "        # Append sample to list.\n",
    "        output.append(bootstrapped_sample)\n",
    "        \n",
    "    # Returns num_B bootstrapped samples in list.\n",
    "    return output\n",
    "\n",
    "# Adapted from Boom D. - NYC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93rYyy1Kdsz1"
   },
   "outputs": [],
   "source": [
    "# Set seed for reproducibility.\n",
    "np.random.seed(42)\n",
    "\n",
    "# Generate five bootstrapped samples from X_train.\n",
    "boot_samp = bootstrap(X_train, 5)\n",
    "\n",
    "# Plot cholesterol level for each bootstrapped sample.\n",
    "for B in range(5):\n",
    "    plt.figure(figsize = (9,6))\n",
    "    plt.hist(boot_samp[B]['Chol'])\n",
    "    plt.title(f'Bootstrapped Sample {B + 1} of Cholesterol Level');"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "jRUpNQ_sdszw",
    "ejWbHtqSdszx",
    "By0ndladdszy"
   ],
   "machine_shape": "hm",
   "name": "models.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
